### 硬件基础

CPU处理器的性能远远高于主内存，一次内存读写操作的时间内，CPU能够处理处理上百条指令。故引入高速缓存。即每个CPU都有属于自己的高速缓存，在做读写操作时不与主内存打交道，而是与缓存打交道。

![8ad3601cffa7913cdc4179260e9988e](C:\Users\dell\AppData\Local\Temp\WeChat Files\8ad3601cffa7913cdc4179260e9988e.png)

### 并发问题的源头

究其本质为CPU、内存、I/O设备的速度差异

为了合理利用cpu资源，平衡这三者之间的速度差异

​	1、cpu增加缓存，以均衡与内存的速度差异

​	2、操作系统增加了进程、线程，以分时复用CPU，进而均衡CPU与I/O设备的速度差异

​	3、编译程序优化指令执行次序

#### 源头之一：缓存导致的可见性问题

引入高速缓存会导致一个问题，即缓存一致性问题。cpu在读取数据时，会将内存中的变量缓存到自己的缓存中。在多核CPU中每个CPU都有自己的缓存，假设CPU1和CPU2都已缓存了一个int i = 0，若A线程运行在CPU1上，此时修改i的值首先会先修改CPU1上的缓存，然后同步到内存中去，然后B线程运行在CPU2上，此时读取i的值，会从缓存中取数据。这种情况下，线程A对变量i的操作对于线程B而言就不具备可见性了。这里会涉及到一个通信问题。即线程A修改了共享变量的值如何通知CPU2更新缓存中共享变量。

总线锁：多核CPU下，当其中一个处理器要对共享数据操作的时候，会在总线上发出一个Lock#信号，这个信号使得其他处理器无法通过总线来访问到共享内存中的数据（即其他处理器不能操作缓存了该共享变量内存地址的缓存，也就是阻塞了其他CPU，使得该处理器可以独享此共享内存），总线锁将CPU和内存之间的通信锁住了，使其他处理器不能操作其他内存地址的数据，所以总线锁的开销比较大，不合适

缓存锁：内存区域如果被缓存在处理器中的缓存行中，并且在Lock期间被锁定，那么当它执行锁操作回写到内存时，不在总线上加锁，而是修改内部的内存地址，基于缓存一致性保证操作的原子性

##### 缓存一致性协议（MESI）

缓存一致性：就是当某块CPU对缓存中的数据操作了之后，就通知其他CPU放弃了存储在他们内部的缓存，或者从主内存中重新读取

MESI协议将缓存分为：Modified(更新)、Exclusive(独占)、Shared(共享)、Invalid(失效)四种状态。

Modified：更新。一个处于M状态的缓存行，必须时刻监听所有视图读取该缓存行对应的主存地址的操作，如果监听到，则必须在此操作执行前把其缓存行中的数据写回CPU

Shared：共享。一个处于S状态的缓存行，必须时刻监听使该缓存行无效或者独享该缓存行的请求，如果监听到，必须将其缓存行状态设置为I

Exclusive：独占。一个处于E状态的缓存行，必须时刻监听其他视图读取该缓存行对应的主存地址的操作，如果监听到，则必须将其缓存行状态设置为S

Invalid：失效。处于此状态的缓存行，如果读取数据时，则需要从内存中读取。

**RFO指令：**当CPU需要写数据时，只有在其缓存行是M或者E的时候才能执行，否则需要发出特殊的RFO指令(Read Or Ownership，这是一种总线事务)，通知其他CPU置缓存无效(I),这种情况下性能开销较大

同时定义了一组消息用于协调各个处理器的读写内存操作。如下：

处理器要执行读写操作时会向总线Bus中发送特定的请求消息，其他处理器嗅探(Snoop也称拦截)总线中由其他处理器所发出的请求消息，并在一定条件下向总线中回复相应的响应消息。

| 消息名                 | 消息类型 | 描述                                                         |
| ---------------------- | -------- | ------------------------------------------------------------ |
| Read                   | 请求     | 通知其他处理器，主内存当前处理器正准备读取某个数据。该消息包含读取数据的内存地址 |
| Read Response          | 响应     | 该消息响应Read消息。其中包含被请求读取的数据，有可能是来自与其他处理器中的高速缓存提供的，也可能是主内存提供的 |
| Invalidate             | 请求     | 通知其他处理器将高速缓存中指定内存地址的缓存条目的状态Flag置为1，即删除指定内存地址的副本数据(逻辑删除) |
| Invalidate Acknowledge | 响应     | 接收到Invalidate消息的处理器必须回复该消息，表示删除了其高速缓存上的副本数据 |
| Read Invalidate        | 请求     | 该消息是Read和Invalidate消息的组合，作用在通知其他处理器当前处理器正正准备更新一个数据(Read-Modify-Write),接收到该消息的处理器必须回复Read Response消息和Invalidate消息 |
| Writeback              | 请求     | 该消息保安写入主内存的数据及其对应的内存地址                 |

##### 写缓存器(Store Buffer)：MESI的优化

通过流程图(见processon)可以发现：如果一个处理器想要进行写内存操作就必须获得相应数据的所有权，即该数据的内存地址所对应的缓存条目的状态必须为E或者M。如果不为E或者M，就需要发送Invalidate或者Read Invalidate消息致使其他处理器上相应的缓存条目失效，从而保证只有当前处理器的缓存中存储了该数据在内存中的副本。

这种机制就导致了一旦处理器进行写内存操作且没有相应的数据所有权，就必须发送消息等待其他处理器回复invalidate Acknowledge消息，而在等待期间，处理器无法继续执行其他命令，降低了处理器的指令执行效率，为了解决这种问题，引入了写缓冲器。

定义：Store Buffer，处理器内部的一个容量比高速缓存还小的储存部件，每个处理器都有其写缓冲器，并且一个处理器无法读取另外一个处理器上的写缓冲器中的内容

引入store buffer之后，处理器会将数据先写入写stroe buffer中并发送Invalidate或Read Invalidate消息。此时处理器已经任务写操作完成不会等待Invalidate Acknowledge消息从而继续执行其他指令。一旦当前处理器接收到了其他处理器的回复消息时，当前处理器会将store buffer中针对相应地址的写操作的结果写入相应的缓存行中。此时对于其他处理器来说写操作才算是完成的。

存储转发：引入store buffer后，处理器在执行读操作时不会直接从缓存中读取数据。而是先在store buffer中查找是否有相应的条目，如果有则直接返回，没有就从缓存中读取数据。



#### 源头之二：线程切换带来的原子性问题

如count+=1操作，实际CPU指令为三条

​	指令1：将变量count从内存加载到CPU的寄存器

​	指令2：在寄存器中执行+1操作

​	指令3：将结果写入内存（缓存机制导致可能写入的时CPU缓存而不是内存）

CPU在做任务切换时，可以发生在任何一条CPU指令，而不是高级语言中的一条语句

eg：线程A和线程B都执行count+=1操作

​		线程A在执行指令1的时候，CPU时间片切换，线程B执行指令1，此时两个线程获得的count都为0，然后各自		执行+1操作，最终结果count的值为1

原子性：将一个或多个操作在CPU执行的过程中不被中断的特性称为原子性

#### 源头之三：编译优化带来的有序性问题

DCL半对象问题

```java
public class Singleton { 
    static Singleton instance; 
    static Singleton getInstance(){ 
        if (instance == null) { 		//代码1
            synchronized(Singleton.class) { 
                if (instance == null) {
                    instance = new Singleton(); 
                } 
            } 
            return instance; 
        }
    }
}
```

想象中的new操作

​	指令1、分配一块内存M

​	指令2、在内存M上初始化Singleton对象

​	指令3、然后M的地址赋给instance变量

实际优化后的new操作

​	指令1、分配一块内存M

​	指令2、将M的地址赋值给instance变量(此时未初始化singleton对象)

​	指令3、最后在内存M上初始化Singleton对象

假设线程A执行到指令2时任务切换到B线程执行，则B线程执行到代码1时发现instance不为null直接返回了，线程这个instance是未初始化的，这个时候访问instance的属性或方法就会报空指针异常

**通过内存屏障禁止了指令重排序**

X86的memory指令包括

lfence(读屏障)：读指令前拆读屏障，可以让高速缓存中的数据失效，重新从主内存加载数据，以保证读取的是最新的数据

sfence(写屏障)：在写指令之后插入写屏障，能让写入缓存的最新数据写回到主内存，以保证写入的数据立刻对其他线程可见

mfence(全屏障)：同时具备读写屏障的能力

1、Store Memory Barrier(写屏障)，告诉处理器在写屏障之前的所有已经存储在存储缓存(store buffer)中的数据同步到主内存，简单来说就是使得写屏障之前的指令的结果对屏障之后的读或写是可见的

2、Load Memory Barrier(读屏障)，处理器在读屏障之后的读操作，都是在读屏障之后执行。配合写屏障，使得写屏障之前的内存更新对于读屏障之后的读操作是可见的

3、Full Memory Barrier(全屏障)，确保屏障前的内存读写操作的结果提交到内存之后，在执行屏障后的读写操作

| 屏障类型   | 指令实例                             | 说明                                                         |
| ---------- | ------------------------------------ | ------------------------------------------------------------ |
| LoadLoad   | Load1;<br />LoadLoad;<br />Load2;    | 保证load1的读操作优先于load2执行                             |
| StoreStore | Store1;<br />StoreStore;<br />Store2 | 保证Store1的写操作优先于Store2执行，并刷新到主内存           |
| LoadStore  | Load1;<br />LoadStore;<br />Store2;  | 保证load1的读操作结束优先于Store2的写操作执行                |
| StoreLoad  | Store1;<br />StoreLoad;<br />Load2;  | 保证Store1的写操作已刷新到主内存之后，load2及其后的读操作才能执行 |

**as-if-serial语义**

所有的动作指令都可以为了优化而被重排序，但是必须保证他们重排序后的结果和程序代码本身的应有结果是一致的。Java编译器、运行时和处理器都会保证单线程下的as-if-serial语义。

```java
public static void main(String[] args) {
        int x, y;
        x = 1;
        try {
            x = 2;
            y = 0 / 0;    
        } catch (Exception e) {
        } finally {
            System.out.println("x = " + x);
        }
 }
```

为了保证as-if-serial语义，Java异常处理机制也会为重排序做一些特殊处理。如上，y=0/0可能优先于x=2前执行，为了保证不至于输出x=1的错误结果，JIT在重排序时会在catch语句块中插入错误补偿代码。将程序恢复到发生异常时应有的状态。

##### 重排序

在执行程序时为了提高性能，编译器和处理器常常会对指令进行重排序

源代码——》编译器优化的重排序——》指令级并行的重排序——》内存系统的重排序——》最终执行指令

* 编译器优化的重排序：编译器在不改变单线程程序语义的前提下，可以重新安排语句的执行顺序
* 指令级的重排序：处理器将多条指令重叠执行。如果不存在数据依赖性，处理器可以改变语句对应机器指令的执行顺序
* 内存系统的重排序：处理器使用缓存和读/写缓冲区，使得加载和存储操作看上去可能是在乱序执行

重排序必须遵循两个规则：数据依赖性，as-if-serial

### JAVA内存模型(JMM)

​	Java内存模型是个抽象的概念，其规范了JVM如何提供按需禁用缓存和编译优化的方法。具体来说，这些方法包	括volatile、sychronized和final三个关键字以及6项Happens-Before规则

JMM定义了线程和主内存之间的抽象关系：

1、线程之间的共享变量存储在主内存中

2、每个线程都有一个私有的本地内存，本地内存中存储了该线程用以读/写共享变量的副本

JMM处理重排序问题：

* 对于编译器，JMM的编译器重排序规则会禁止特定类型的编译器重排序
* 对于处理器重排序，JMM重排序规则会要求Java编译器在生成指令序列时，插入特定类型的内存屏障指令，来禁止特定类型的处理器重排序
* JMM根据代码中的关键字和JUC包下的一些具体类来插入内存屏障

#### Happens-Before规则：前面一个操作的结果对后续操作是可见的

1、程序的顺序行规则：在一个线程中，按照程序顺序，前面的操作happpens-before于后续的任意操作

2、volatile变量规则：对一个volatile变量的写操作，Happens-Before于后续对这个volatile变量的读操作

**volatile如何保证有序性：**一个变量被volatile修饰之后

​		1）对于写操作：对变量更改完之后，要立刻写回到主内存中

​		2）对于读操作：对变量的读取，要从主内存中读，而不是缓存

3、传递性：若A Happens-Before B，且B Happens——Before C,那么A Happens—Before C

4、synchronzied锁规则：对一个锁的解锁Happens-Before于后续对这个锁的加锁

5、线程start()规则：主线程A启动子线程B后，子线程B能够看到主线程在启动子线程B前的操作

6、线程join()规则：主线程A等待子线程B完成(主线程A通过调用子线程B的join()方法实现)，当子线程B完成后(主线程A中的join()方法返回)，主线程能够看到子线程的对共享变量的操作

#### volatile

1）保证可见性

StoreLoad屏障：将当前处理器缓存行的数据写回到系统内存，这个写回内存的操作回事在其他CPU里缓存了该内存地址的数据无效

2）保证有序性（禁止重排序规则）

JMM通过内存屏障指令来禁止特定类型的重排序。即编译器在生成字节码时，在volatile变量操作前后的指令序列中插入内存屏障来禁止特定类型的重排序。

volatile内存屏障插入策略：

* 在每个volatile写操作的前面插入一个StoreStore屏障
* 在每个volatile写操作的后面插入一个StoreLoad屏障
* 在每个volatile读操作的前面插入一个LoadLoad屏障
* 在每个volatile读操作的后面插入一个LoadStore屏障

3）不保证原子性



#### final关键字

final修饰变量时，初衷是告诉编译器：这个变量生而不变，可以可劲儿优化。保证一个对象的构造方法结束前，所有final成员变量都必须完成初始化

1）当final修饰基本数据类型时，表示该基本数据类型的值一旦在初始化后便不能发生变化

2）修饰一个引用类型时，则在对其初始化之后便不能再让其指向其他对象了，但是该引用所指向的对象的内容是可以发生变化的